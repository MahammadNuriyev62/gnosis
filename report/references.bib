@inproceedings{stanton2021does,
  title={Does Knowledge Distillation Really Work?},
  author={Stanton, Samuel and Izmailov, Pavel and Kirichenko, Polina and Alemi, Alexander A and Wilson, Andrew Gordon},
  booktitle={Advances in Neural Information Processing Systems},
  volume={34},
  pages={6906--6919},
  year={2021}
}

@article{hinton2015distilling,
  title={Distilling the Knowledge in a Neural Network},
  author={Hinton, Geoffrey and Vinyals, Oriol and Dean, Jeff},
  journal={arXiv preprint arXiv:1503.02531},
  year={2015}
}

@inproceedings{kornblith2019similarity,
  title={Similarity of Neural Network Representations Revisited},
  author={Kornblith, Simon and Norouzi, Mohammad and Lee, Honglak and Hinton, Geoffrey},
  booktitle={International Conference on Machine Learning},
  pages={3519--3529},
  year={2019}
}

@inproceedings{zhang2018mixup,
  title={mixup: Beyond Empirical Risk Minimization},
  author={Zhang, Hongyi and Cisse, Moustapha and Dauphin, Yann N and Lopez-Paz, David},
  booktitle={International Conference on Learning Representations},
  year={2018}
}

@inproceedings{he2016identity,
  title={Identity Mappings in Deep Residual Networks},
  author={He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
  booktitle={European Conference on Computer Vision},
  pages={630--645},
  year={2016}
}

@inproceedings{bucilua2006model,
  title={Model Compression},
  author={Bucilu{\u{a}}, Cristian and Caruana, Rich and Niculescu-Mizil, Alexandru},
  booktitle={ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
  pages={535--541},
  year={2006}
}

@inproceedings{furlanello2018born,
  title={Born Again Neural Networks},
  author={Furlanello, Tommaso and Lipton, Zachary C and Tschannen, Michael and Itti, Laurent and Anandkumar, Anima},
  booktitle={International Conference on Machine Learning},
  pages={1607--1616},
  year={2018}
}

@inproceedings{cho2019efficacy,
  title={On the Efficacy of Knowledge Distillation},
  author={Cho, Jang Hyun and Hariharan, Bharath},
  booktitle={IEEE International Conference on Computer Vision},
  pages={4794--4802},
  year={2019}
}

@inproceedings{romero2015fitnets,
  title={FitNets: Hints for Thin Deep Nets},
  author={Romero, Adriana and Ballas, Nicolas and Kahou, Samira Ebrahimi and Chassang, Antoine and Gatta, Carlo and Bengio, Yoshua},
  booktitle={International Conference on Learning Representations},
  year={2015}
}

@inproceedings{lakshminarayanan2017simple,
  title={Simple and Scalable Predictive Uncertainty Estimation using Deep Ensembles},
  author={Lakshminarayanan, Balaji and Pritzel, Alexander and Blundell, Charles},
  booktitle={Advances in Neural Information Processing Systems},
  volume={30},
  year={2017}
}

@article{tang2020understanding,
  title={Understanding and Improving Knowledge Distillation},
  author={Tang, Jiaxi and Shivanna, Rakesh and Zhao, Zhe and Lin, Dong and Singh, Anima and Chi, Ed H and Jain, Sagar},
  journal={arXiv preprint arXiv:2002.03532},
  year={2020}
}

@inproceedings{tian2020contrastive,
  title={Contrastive Representation Distillation},
  author={Tian, Yonglong and Krishnan, Dilip and Isola, Phillip},
  booktitle={International Conference on Learning Representations},
  year={2020}
}

@article{gou2021knowledge,
  title={Knowledge Distillation: A Survey},
  author={Gou, Jianping and Yu, Baosheng and Maybank, Stephen J and Tao, Dacheng},
  journal={International Journal of Computer Vision},
  volume={129},
  pages={1789--1819},
  year={2021}
}

@inproceedings{allen2023towards,
  title={Towards Understanding Ensemble, Knowledge Distillation and Self-Distillation in Deep Learning},
  author={Allen-Zhu, Zeyuan and Li, Yuanzhi},
  booktitle={International Conference on Learning Representations},
  year={2023}
}
