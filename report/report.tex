\documentclass[10pt]{article}
\usepackage[accepted]{icml2026}
\makeatletter
\renewcommand{\ICML@appearing}{}
\makeatother

\usepackage{amsmath,amssymb}
\usepackage{booktabs}
\usepackage{subcaption}
\usepackage{graphicx}
\usepackage{multirow}
\usepackage{float}

\icmltitlerunning{Replication Study: Does Knowledge Distillation Really Work?}

\begin{document}

\twocolumn[
\icmltitle{Replication Study: Does Knowledge Distillation Really Work?}

\begin{icmlauthorlist}
\icmlauthor{Rasul Alakbarli}{univ}
\icmlauthor{Mahammad Nuriyev}{univ}
\icmlauthor{Petko Petko}{univ}
\icmlauthor{Dmitrije Zdrale}{univ}
\end{icmlauthorlist}

\icmlaffiliation{univ}{Course Project, Learning Theory and Advanced Machine Learning, February 2026}

\icmlcorrespondingauthor{Mahammad Nuriyev}{mahammad.nuriyev@universite-paris-saclay.fr}

\icmlkeywords{knowledge distillation, fidelity, replication study}

\vskip 0.3in

\begin{abstract}
We replicate key experiments from \emph{``Does Knowledge Distillation Really Work?''} by Stanton et al.\ (NeurIPS 2021). The paper investigates a fundamental tension in knowledge distillation: while distilled students often achieve good generalization, they exhibit surprisingly poor \emph{fidelity} to their teachers' predictions. We reproduce experiments on CIFAR-100 using PreResNet-20 architectures, examining self-distillation, ensemble distillation, initialization proximity, and representational similarity via Centered Kernel Alignment (CKA). Our results confirm the paper's central finding: knowledge distillation improves student generalization but transfers limited knowledge from teacher to student, with optimization difficulties being the primary cause of low fidelity. We additionally compute pairwise agreement between independently trained teachers to establish how much they already agree without distillation, finding that distillation improves fidelity by only $\sim$8 percentage points over this baseline. We document the practical challenges of replication with modern software stacks.
\end{abstract}
]

\printAffiliationsAndNotice{}

\section{Introduction}

Knowledge distillation (KD), introduced by \citet{hinton2015distilling} and rooted in earlier model compression work by \citet{bucilua2006model}, is one of the most widely-used techniques for deploying neural networks in resource-constrained settings. The core idea is to train a smaller \emph{student} network to emulate a larger, more capable \emph{teacher} model by matching the teacher's soft output distribution rather than (or in addition to) the hard training labels. This technique has found broad application in model deployment, ensemble compression, and continual learning.

The conventional understanding of KD is that the student learns a high-fidelity representation of the teacher's knowledge through its soft labels, namely that the ``dark knowledge'' encoded in the teacher's output distribution over non-target classes provides rich supervisory signal that hard labels alone cannot capture \citep{hinton2015distilling}. This narrative has motivated numerous extensions, including intermediate representation matching \citep{romero2015fitnets}, contrastive distillation \citep{tian2020contrastive}, and self-distillation \citep{furlanello2018born}, each aiming to transfer more information from teacher to student.

However, \citet{stanton2021does} challenge this narrative by carefully distinguishing between two properties that are often conflated:

\emph{Generalization} refers to the student's performance on unseen test data, measured by accuracy, negative log-likelihood (NLL), and expected calibration error (ECE). \emph{Fidelity} refers to the degree to which the student's predictions match the teacher's, measured by top-1 agreement and predictive KL divergence.

The key finding is that while KD often improves generalization, it frequently fails to produce high-fidelity students, even when the student has identical capacity to the teacher (self-distillation). This contradicts the standard narrative: if the student is simply ``learning the teacher's knowledge,'' it should reproduce the teacher's predictions. The fact that it does not suggests that KD's benefit may come from regularization or label smoothing rather than genuine knowledge transfer.

The authors investigate multiple hypotheses: (1) \emph{insufficient data}, that perhaps more distillation data would improve fidelity; (2) \emph{optimization difficulties}, that perhaps standard optimizers cannot find the teacher's solution; and (3) \emph{inductive biases}, that perhaps differences in augmentation or architecture prevent matching. They ultimately conclude that optimization is the primary bottleneck, demonstrating a sharp phase transition in the loss landscape that determines whether the student converges to the teacher's basin.

These findings have significant practical implications. Model compression via distillation is widely used in industry: deploying ensemble predictions through a single student, compressing large language models for edge devices, and transferring knowledge between modalities. If the student does not faithfully reproduce the teacher's behavior, then the compressed model may have different failure modes, different calibration properties, and different biases than the original, even if aggregate metrics (like accuracy) look similar. For safety-critical applications, understanding whether distillation preserves the teacher's specific behavior (not just its aggregate performance) is essential.

In this replication study, we reproduce the core experiments using CIFAR-100 with PreResNet-20 architectures. We focus on the experiments that most directly support the paper's key claims: self-distillation fidelity (Figure 1a in the original), ensemble distillation fidelity (Figure 1b), student initialization proximity and loss landscape structure (Figure 6b), and CKA representational similarity analysis (Table 1). We additionally contribute an \emph{additional baseline analysis} not present in the original work: computing pairwise agreement between independently trained teacher models. This baseline provides essential context for interpreting the fidelity gap; specifically, it reveals how much agreement improvement distillation provides beyond what independent training achieves.

\section{Background}

\subsection{Knowledge Distillation}

In the supervised classification setting with $c$ classes, a classifier $f: \mathcal{X} \times \Theta \rightarrow \mathbb{R}^c$ produces logits defining a predictive distribution $\hat{p}(y=i|x) = \sigma_i(f(x, \theta))$ via the softmax $\sigma$.

Given a trained teacher with parameters $\theta_t$, \citet{hinton2015distilling} proposed minimizing:
\begin{equation}
    \mathcal{L}_s = \alpha \mathcal{L}_{\text{NLL}} + (1-\alpha) \mathcal{L}_{\text{KD}},
\end{equation}
where $\mathcal{L}_{\text{NLL}} = -\log \hat{p}(y=y^*|x)$ is the standard cross-entropy and:
\begin{equation}
    \mathcal{L}_{\text{KD}}(z_s, z_t) = -\tau^2 \sum_{j=1}^{c} \sigma_j\!\left(\frac{z_t}{\tau}\right) \log \sigma_j\!\left(\frac{z_s}{\tau}\right),
\end{equation}
with $z_s, z_t$ being student and teacher logits and $\tau > 0$ a temperature hyperparameter. The $\tau^2$ factor ensures gradients scale appropriately. As $\tau \rightarrow \infty$, the softmax approaches a uniform distribution, and gradient information is dominated by relative logit magnitudes.

Following the original paper, we set $\alpha = 0$ in all main experiments to isolate the effect of distillation without confounding from hard labels. With $\alpha = 0$, any accuracy improvement is entirely attributable to the teacher's soft labels.

\subsection{Metrics}

We report both \emph{generalization} and \emph{fidelity} metrics. The generalization metrics are \textbf{top-1 accuracy} (fraction of test examples correctly classified), \textbf{negative log-likelihood} (NLL; average $-\log \hat{p}(y=y^*|x)$ on the test set, measuring the quality of the full predictive distribution), and \textbf{expected calibration error} (ECE; whether predicted confidences match true correctness probabilities, computed using 10 equal-width bins).

The fidelity metrics, which are the primary focus of this study, are \textbf{top-1 agreement} (fraction of test examples where teacher and student predict the same class: $\frac{1}{N}\sum_{i=1}^{N} \mathbb{1}[\hat{y}_t(x_i) = \hat{y}_s(x_i)]$), \textbf{predictive KL} (average KL divergence from teacher to student: $\frac{1}{N}\sum_{i=1}^{N} \text{KL}(\hat{p}_t(\cdot|x_i) \| \hat{p}_s(\cdot|x_i))$, capturing differences in the full distribution), and \textbf{CKA} (Centered Kernel Alignment~\citep{kornblith2019similarity} between intermediate representations, measuring structural similarity in learned feature spaces).
A perfect distillation would yield 100\% agreement and zero KL divergence. The central question is how far real distillation falls short. Importantly, the paper argues that agreement and accuracy can be \emph{negatively} correlated in the self-distillation setting: the student's best accuracy comes not from matching the teacher but from finding its own, different solution.

\subsection{Ensemble Teachers}

Deep ensembles \citep{lakshminarayanan2017simple} combine $m$ independently trained models. The ensemble teacher logits are:
\begin{equation}
z_t = \log\!\left(\frac{1}{m}\sum_{i=1}^{m} \sigma(z_i)\right),
\end{equation}
corresponding to the model-averaged distribution. Ensemble distillation is practically important because ensembles are expensive to deploy \citep{hinton2015distilling}.

\subsection{Self-Distillation}

In \emph{self-distillation}, teacher and student share the same architecture and capacity. \citet{furlanello2018born} showed self-distillation can \emph{improve} accuracy beyond the teacher, a phenomenon they called ``Born Again Neural Networks.'' This creates a paradox: the student supposedly learns from the teacher yet outperforms it. Stanton et al.\ resolve this by showing the student is \emph{not} faithfully learning the teacher; it improves precisely because it diverges from the teacher's predictions. The soft labels act as a form of regularization that guides the student to a different (and sometimes better) solution, rather than transferring the teacher's specific decision boundaries.

This perspective reframes self-distillation as a \emph{training strategy} rather than a \emph{knowledge transfer} method. The teacher provides useful training signal, but the student uses this signal to find its own solution rather than reproducing the teacher's. Understanding this distinction is crucial for practitioners who expect the distilled model to behave ``like the teacher but smaller.''

\subsection{CKA: Centered Kernel Alignment}

Two networks may produce similar predictions while organizing their internal representations differently, or conversely, learn similar representations yet disagree on outputs. CKA \citep{kornblith2019similarity} quantifies the similarity of intermediate representations between networks, invariant to orthogonal transformations and isotropic scaling. Given the same inputs, we extract activation matrices from corresponding layers of the teacher and student and compute, for representation matrices $X$ and $Y$:
\begin{equation}
\text{CKA}(X, Y) = \frac{\text{HSIC}(K_X, K_Y)}{\sqrt{\text{HSIC}(K_X, K_X) \cdot \text{HSIC}(K_Y, K_Y)}},
\end{equation}
where HSIC is the Hilbert-Schmidt Independence Criterion. CKA ranges from 0 (dissimilar) to 1 (identical up to invariances). We compute CKA at each of PreResNet-20's three residual stages.

\section{Experimental Setup}

\subsection{Architecture}

We use Pre-activation ResNet-20 (PreResNet-20) \citep{he2016identity} throughout, with $\sim$0.27M parameters, three residual stages with [16, 32, 64] filters, and pre-activation batch normalization. Input size is $32 \times 32$ (CIFAR-100).

\subsection{Training Configuration}

\textbf{Teachers} are trained for 200 epochs with SGD (momentum=0.9, Nesterov, weight decay $10^{-4}$), learning rate 0.1 with cosine annealing, batch size 256. We train 5 teachers with different random seeds.

\textbf{Students} are trained for 300 epochs with learning rate $5 \times 10^{-2}$, cosine annealing to $10^{-6}$, SGD with Nesterov momentum, weight decay $10^{-4}$, batch size 128. Default: temperature $\tau=4$, $\alpha=0$ (pure soft labels).

\textbf{Data augmentation}: random crops ($32 \times 32$, 4px padding) and random horizontal flips. Pixels normalized to $[-1, 1]$ via unitcube normalization. These settings serve as the default configuration; individual experiments modify specific factors as described in their respective sections.

\subsection{Teacher Training Results}

We trained 5 PreResNet-20 teachers on CIFAR-100 (Table~\ref{tab:teachers}).

\begin{table}[t]
\centering
\caption{Teacher model results (CIFAR-100, 200 epochs).}
\label{tab:teachers}
\begin{small}
\begin{tabular}{lcc}
\toprule
Model & Test Acc (\%) & Train Acc (\%) \\
\midrule
Teacher 0 & 65.22 & 94.18 \\
Teacher 1 & 65.34 & 93.86 \\
Teacher 2 & 64.83 & 93.56 \\
Teacher 3 & 64.63 & 93.36 \\
Teacher 4 & 65.21 & 93.76 \\
\midrule
Mean & 65.05 $\pm$ 0.27 & 93.74 $\pm$ 0.28 \\
\bottomrule
\end{tabular}
\end{small}
\end{table}

Our teacher accuracy ($\sim$65\%) is lower than the $\sim$70.5\% in the original paper, likely due to PyTorch version differences (2.10 vs.\ 1.10) affecting cuDNN algorithms and batch normalization numerics. This gap does not affect qualitative findings about fidelity versus generalization, which are the paper's core contribution.

\subsection{Deviations from Original Setup}

The original paper's codebase targets Python 3.8 and PyTorch 1.10, which are difficult to reproduce exactly with current CUDA drivers and hardware. We use modern versions and document the resulting differences in Table~\ref{tab:deviations}.

\begin{table}[t]
\centering
\caption{Key deviations from the original setup.}
\label{tab:deviations}
\begin{small}
\begin{tabular}{lll}
\toprule
Aspect & Original & Ours \\
\midrule
Python & 3.8 & 3.12 \\
PyTorch & 1.10+cu113 & 2.10+cu128 \\
GPU & Not specified & RTX 4060 Ti \\
Norm (Exp.\ 3) & LayerNorm & BatchNorm \\
$\lambda$ values & Dense sweep & Sparse \\
\bottomrule
\end{tabular}
\end{small}
\end{table}

A notable deviation concerns the initialization proximity experiment (Exp.\ 3) only: the original paper specifies LayerNorm for this experiment because BatchNorm statistics depend on the training data, making interpolation between models less straightforward. All other experiments use BatchNorm in both the original work and our replication. However, the released codebase uses BatchNorm throughout with no LayerNorm variant provided, and we treated the codebase as the source of truth. As we show in Figure~\ref{fig:initialization}, the phase transition is still clearly observed with BatchNorm, suggesting this deviation does not undermine the core finding. Additionally, the original paper sweeps $\lambda$ densely to produce a smooth phase transition curve, whereas we test only a few values due to compute constraints, as each requires a full training run.

\subsection{Infrastructure}

As no team member had access to a local GPU, we allocated a budget of 15 EUR to rent an RTX 4060 Ti machine through the vast.ai cloud platform. Each team member accessed the machine via SSH with a separate Linux user account, providing isolation and traceability for all operations. We used Git and GitHub for version control to coordinate the work.\footnote{Repository: \url{https://github.com/MahammadNuriyev62/gnosis}}


\bibliographystyle{icml2026}
\bibliography{references}

\end{document}
